{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importation of Data manipulation libraries\nimport pandas as pd\nimport numpy as np\n# Importation of visualisation libraries\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Machine Learning Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV,train_test_split,RandomizedSearchCV\n# Imputing missing values and scaling values\nfrom sklearn.impute import SimpleImputer\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Objective of this task is to predict permit issue times of various building permits so as to identify which ones matter more and also be able to draw conclusions on the city's development plans based on the San Francisco Building Permits data.\nIt's a Regression problems since the permit issue time will be a continuous variable.\nIt's a supervised Machine learning problem since we have access to the features and the target variable"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',None)\nPmtsdata = pd.read_csv('../input/building-permit-applications-data/Building_Permits.csv',\n                      index_col='Permit Number',parse_dates=['Filed Date','Issued Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Phase 1:Data cleaning and formatting.\nPmtsdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gives us some more information about the dataframe .\nPmtsdata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploring missing values\nmissing_values_counts = Pmtsdata.isnull().sum()\nprint(missing_values_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if we were to drop columns with atleast one missing value\ncolumns_without_missing_values= Pmtsdata.dropna(axis=1)\ncolumns_without_missing_values.head()\n#Alot of data is lost. Out of 42 columns only 11 are retained, 31 columns are lost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if we were to drop rows with atleast one missing value\nrows_without_missing_values= Pmtsdata.dropna(axis=0)\nrows_without_missing_values.head()\n#All the data would be lost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets explore the possibility of dropping columns by a given percentage threshold of missings values.\nmiss_val_per_column = Pmtsdata.isnull().sum()/len(Pmtsdata)\nmiss_val_per_column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mis_val_centage = (miss_val_per_column*100).round(3)\nmis_val_centage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mis_val_table = pd.concat([miss_val_per_column,mis_val_centage,],axis=1)\nmis_val_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_table = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : 'Percentage'})\nnew_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set our threshold to remove columns with 80% missing values.\nmissing_columns = list(new_table[new_table['Percentage']>80].index)\nprint('We will remove %d columns'%len(missing_columns))\nprint('The columns to remove are \\n %s'%missing_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.drop(columns=list(missing_columns),inplace=True)\n#Now left with 34 columns out of 42.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standard machine learning models cannot deal with missing values, and which means we have to find a way to fill these in or disard any features with missing values. Since we already removed features with more than 80% missing values in the dateframe,\nwe have a considerable number of rows process, we can decide to drop rows with missing values so as to remain with data which suitable to our models**."},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.dropna(axis=0,inplace=True) # we can drop rows with missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.isnull().any().sum() #code shows we now have no missing values in the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.shape # we are still left with a considerable chunk of data to build our models.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DUPLICATE CHECK.\nNewPmtdata=Pmtsdata.copy()\nNewPmtdata.drop_duplicates(subset=None, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NewPmtdata.shape  # no duplicate rows present","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we shall compute how long it takes for a building permit to be issued by creating the target variable time in days from the Filed Date and the Issued Date **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a new column time in days taken to receive a permit from the filed date and issued date\nPmtsdata['Time_in_Days']= Pmtsdata['Issued Date'].sub(Pmtsdata['Filed Date'],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata['Time_in_Days'].head()# Asnap shot on the time in days taken.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert the time in days from Datetime format to integer\nPmtsdata['Time_in_Days']=Pmtsdata['Time_in_Days'].dt.days","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Phase 2: \nNext step is to do exploratory Data analysis whose purpose is to find anormalies, trends, partners and relationships that can be used to inform modeling decisions such as which features to use with strong correlation. This helps us to determine what our data tell us**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata['Time_in_Days'].describe() # exploring our target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.loc[Pmtsdata['Time_in_Days']== 1262] #exploring a row of interest\n# Permit type with longest issue time is of category 3 which involves additions, alterations or repairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.loc[Pmtsdata['Time_in_Days']== 0][:5] \n# Permit type with shortest issue time are majorly of category 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_in_days=Pmtsdata['Time_in_Days'].value_counts(sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(count_in_days.head()) # The highest number of permits are processed in a few hours.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we also have a number single permit recordes with high processing times.\nprint(count_in_days.tail())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(count_in_days, bins = 50, edgecolor = 'k');\nplt.xlabel('Time Taken in Days'); plt.ylabel('Count of records'); \nplt.title('Count of days distribution')\nplt.show()\n#It's a highly skewed distribution of the time taken for various permits to be processed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=count_in_days,shade=False,alpha=0.8)\nplt.show() #gives a clear representation of skewness in the time in days taken by various permits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing some categorical columns:Permit Type,Permit Type Definition,Current Status, Existing Construction Type **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets do some bivariate plotting\nfig=plt.figure(figsize=(8,6))\nsns.barplot(x=Pmtsdata['Permit Type'],y=Pmtsdata['Time_in_Days'],hue='Permit Type',data=Pmtsdata)\nplt.xlabel('Permit Type')\nplt.xticks(size=14)\nplt.ylabel('Time in Days')\nplt.title('Time in days for various  permit types')\n\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be observed that Permit type of category 8 take the least time to process,followed by permit type 3 while permit type 2  take the most time on average to process."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(8, 8))\nplt.hist(Pmtsdata['Permit Type'], bins = 20, edgecolor = 'black');\nplt.xlabel('Permit type'); \nplt.ylabel('Count'); plt.title('Permit Type Distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histogram above shows that other permit types categories 2,4,5,6,7 do not have a significant count of records.\nPermit types categories 8 and 3 are the common permit types to be issued.\n\nLets Explore the current status variable with emphasis to key values such as issued, revoked and incomplete permit applicatons."},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtdata1 =Pmtsdata[Pmtsdata['Current Status'].isin(['issued','revoked','incomplete']) ] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtdata1['Current Status'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot both together to compare\nfig=plt.figure(figsize=(8,6))\nsns.catplot(x='Permit Type',hue='Current Status',kind='count',data=Pmtdata1)\nplt.xlabel('Current Status')\nplt.xticks()\nplt.ylabel('Number of permits')\nplt.title('Current status  permit types')\nplt.xticks()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **The Bar Graph above shows that there are more permits that are issued, a few permits that are revoked and a slight number of them that are incomplete meaning that there are more chances for a permit to be issued once all required documents have been submitted.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A Count plot showing distribution of Permit Type Definition with permit types\nfig=plt.figure(figsize=(8,6))\nsns.catplot(y='Permit Type Definition',hue='Permit Type',kind='count',data=Pmtsdata)\nplt.xlabel('Permit counts')\nplt.xticks()\nplt.ylabel('Permit Type Definition')\nplt.title('Number of permits per permit type definition')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Graph above shows that Alterations permits are the common permits type definition in the data with other permits definitions greatly skewed.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot both together to compare\nfig=plt.figure(figsize=(8,6))\nsns.catplot(x='Permit Type',hue='Existing Construction Type',kind='count',data=Pmtsdata)\nplt.xlabel('Permit Types')\nplt.ylabel('Count of Existing Construction types')\nplt.title('Count of Permit types for existing construction types')\nplt.xticks()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The most construction types permits issued belong to Existing Construction type 5.0 which is reflected across the major permit types 8 and 3, then followed by construction type 1.0 ,3.0,2.0 and least number of permits belong to construction type 4.0.**\n\n**Visualise highly skewed columns which include: estimated cost, revised cost, plansets. and decrease on thier skewness so as to produce better models.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Estimated Cost'] ,shade=False,alpha=0.8)\nplt.show()\n#graph below shows a skewed distribution to right","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate first and third quartile\nfirst_quartile = Pmtsdata['Estimated Cost'].describe()['25%']\nthird_quartile = Pmtsdata['Estimated Cost'].describe()['75%']\n\n# Interquartile range\niqr = third_quartile - first_quartile\n\n# Remove outliers\nPmtsdata = Pmtsdata[(Pmtsdata['Estimated Cost'] > (first_quartile - 3 * iqr)) &\n            (Pmtsdata['Estimated Cost'] < (third_quartile + 3 * iqr))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Estimated Cost'] ,shade=False,alpha=0.8)\nplt.show() #this shows a better shape with decreased skewness","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Plansets'],shade=False,alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate first and third quartile\nfirst_quartile = Pmtsdata['Plansets'].describe()['25%']\nthird_quartile = Pmtsdata['Plansets'].describe()['75%']\n\n# Interquartile range\niqr = third_quartile - first_quartile\n\n# Remove outliers\nPmtsdata = Pmtsdata[(Pmtsdata['Plansets'] > (first_quartile - 3 * iqr)) &\n            (Pmtsdata['Plansets'] < (third_quartile + 3 * iqr))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Plansets'],shade=False,alpha=0.8)\nplt.show()\n#shows decrease in skewness","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Revised Cost'],shade=False,alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate first and third quartile\nfirst_quartile = Pmtsdata['Revised Cost'].describe()['25%']\nthird_quartile = Pmtsdata['Revised Cost'].describe()['75%']\n\n# Interquartile range\niqr = third_quartile - first_quartile\n\n# Remove outliers\nPmtsdata = Pmtsdata[(Pmtsdata['Revised Cost'] > (first_quartile - 3 * iqr)) &\n            (Pmtsdata['Revised Cost'] < (third_quartile + 3 * iqr))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Revised Cost'],shade=False,alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Existing Units'],shade=False,alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate first and third quartile\nfirst_quartile = Pmtsdata['Existing Units'].describe()['25%']\nthird_quartile = Pmtsdata['Existing Units'].describe()['75%']\n\n# Interquartile range\niqr = third_quartile - first_quartile\n\n# Remove outliers\nPmtsdata = Pmtsdata[(Pmtsdata['Existing Units'] > (first_quartile - 3 * iqr)) &\n            (Pmtsdata['Existing Units'] < (third_quartile + 3 * iqr))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Existing Units'],shade=False,alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Proposed Units'],shade=False,alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate first and third quartile\nfirst_quartile = Pmtsdata['Proposed Units'].describe()['25%']\nthird_quartile = Pmtsdata['Proposed Units'].describe()['75%']\n\n# Interquartile range\niqr = third_quartile - first_quartile\n\n# Remove outliers\nPmtsdata =Pmtsdata[(Pmtsdata['Proposed Units'] > (first_quartile - 3 * iqr)) &\n            (Pmtsdata['Proposed Units'] < (third_quartile + 3 * iqr))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=Pmtsdata['Proposed Units'],shade=False,alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.shape #New shape after removing outliers.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix =Pmtsdata.corr()['Time_in_Days'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the most negative and least correlations\ncorrelation_matrix.head(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix.tail(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types = (Pmtsdata['Permit Type'].value_counts())\ntypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types = list(types[types.values > 300].index)\ntypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(8,6))\nplt.hist(types,bins=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of distribution of time in days for permit types\nplt.figure(figsize=(10,8))\n\n# Plot each permit type\nfor p_type in types:\n    \n    # Select the permit type\n    subset = Pmtsdata[Pmtsdata['Permit Type'] == p_type]\n    \n    # Density plot of Energy Star scores\n    sns.kdeplot(subset['Time_in_Days'],\n               label = p_type, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('Time in Days', size = 8); plt.ylabel('Density', size = 8); \nplt.title('Density Plot of Time in Days by Permit Type', size = 8);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = ['Permit Type','Street Number','Existing Construction Type','Zipcode','Supervisor District']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata[cat_col]=Pmtsdata[cat_col].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.dtypes # to verify that our data types","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping of Filed Date and Issued dates since they are nolonger informative in our modelling,\nWe drop Record ID since  we already have a unique record identifier,\nwe also drop locationn since it's not so informative as we already have a supervisor district which gives us a hint on the area the building is located,\nwe also drop columns that could be colinear to other columns in the data and hence lead to overfitting\nThese include:Number of existing stories,estimated cost,existing units**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.drop(columns =list(['Filed Date','Issued Date','Record ID','Location',\n                           'Number of Existing Stories','Estimated Cost',\n                           'Existing Units','Current Status']),axis=1,inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.head(1)# shows that we have dropped the above mentioned columns.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pmtsdata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=pd.DataFrame(Pmtsdata['Time_in_Days'])\nX =Pmtsdata.drop(['Time_in_Days'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality\ncategorical_cols = [cname for cname in X.columns \n                    if X[cname].nunique() < 10 and X[cname].dtype == \"object\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_colsOH= pd.get_dummies(X[categorical_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_colsOH.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_colsOH.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import  StandardScaler\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns \n                  if X[cname].dtype in ['int64', 'float64']]\n\nscaler = StandardScaler()\nX[numerical_cols] = scaler.fit_transform(X[numerical_cols] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The code below shows that the numerical columns have minimum skewness.\nX[numerical_cols].skew(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_data = pd.DataFrame(X[numerical_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_data.head()# taking a snapshot of the numerical data columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features =pd.concat([categorical_colsOH, numerical_data],axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert y to one-dimensional array (vector)\ny = np.array(y).reshape((-1, ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Break off test set from training data\nX_train, X_test, y_train, y_test =train_test_split(features,y,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate mean absolute error\ndef mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we can make the median guess and evaluate it on the test set to obtain our baseline model\nbaseline_guess = np.median(y)\n#This shows our average estimate on the test set is off by about 13 points. \n#The Time in days can take on any values, the average error from a naive method if about 13%. \n#The naive method of guessing the median training value provides us a low baseline for our models to beat\n\nprint('The baseline guess of number of days taken: %0.2f days' % baseline_guess)\nprint(\"Baseline Performance on the test set: MAE = %0.4f\" % mae(y_test, baseline_guess))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to  train a given  model and evaluate it on the test set\ndef fit_and_evaluate(model):\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions and evalute\n    model_pred = model.predict(X_test)\n    model_mae = mae(y_test, model_pred)\n    \n    # Return the performance metric\n    return model_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr_mae = fit_and_evaluate(lr)\n\nprint('Linear Regression Performance on the test set: MAE = %0.4f' % lr_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestRegressor(random_state=0)\nrandom_forest_mae = fit_and_evaluate(random_forest)\n\nprint('Random Forest Regression Performance on the test set: MAE = %0.4f' % random_forest_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boosted = GradientBoostingRegressor(random_state=4)\ngradient_boosted_mae = fit_and_evaluate(gradient_boosted)\n\nprint('Gradient Boosted Regression Performance on the test set: MAE = %0.4f' % gradient_boosted_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees used in the boosting process\nn_estimators = [100, 500, 900, 1100, 1500]\n\n#loss function to be minimized\nloss = ['ls', 'lad', 'huber']\n\n# Maximum depth of each tree\nmax_depth = [2, 3, 5, 10, 15]\n#how much the contribution of each tree will shrink.\n\nlearning_rate = [0.005,0.01,0.05,0.1,0.5]\n\n# Minimum number of samples to split a node\nmin_samples_split = [2, 4, 6, 10]\n\n# Maximum number of features to consider for making splits\nmax_features = ['auto', 'sqrt', 'log2', None]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the grid of hyperparameters to search\nhyperparameter_grid = {'loss': loss,\n                       'learning_rate':learning_rate,\n                       'n_estimators': n_estimators,\n                       'max_depth': max_depth,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model for optimization\nmodel = GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=5, n_iter=30, \n                               scoring = 'neg_mean_absolute_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_cv.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all of the cv results and sort by the test performance\nrandom_results = pd.DataFrame(random_cv.cv_results_).sort_values('mean_test_score', ascending = False)\nrandom_results.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a range of trees to evaluate\ntrees_grid = {'n_estimators': [100,200,300,400,500, 1500, 2000,2500]}\nmodel =  GradientBoostingRegressor( max_depth =3,\n                                   loss='lad',\n                                   learning_rate=0.5,\n                                  min_samples_split = 6,\n                                  max_features = 'log2',\n                                  random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search Object using the trees range and the random forest model\ngrid_search = GridSearchCV(estimator = model, param_grid=trees_grid, cv = 5, \n                           scoring = 'neg_mean_absolute_error', verbose = 1,\n                           n_jobs = -1, return_train_score = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the results into a dataframe\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Plot the training and testing error vs number of trees\nplt.figure(figsize=(8, 8))\nplt.style.use('fivethirtyeight')\nplt.plot(results['param_n_estimators'], -1 * results['mean_test_score'], label = 'Test_Err')\nplt.plot(results['param_n_estimators'], -1 * results['mean_train_score'], label = 'Train_Err')\nplt.xlabel('Number of Trees'); plt.ylabel('Mean Abosolute Error'); plt.legend(\"best\");\nplt.title('Performance vs Number of Trees');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.sort_values('mean_test_score', ascending = False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#default model\ndefaultmodelGBR = GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the best model\nfinal_modelGBR = grid_search.best_estimator_\nfinal_modelGBR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\n# Select the best parameters for best estimator\npprint(grid_search.best_estimator_.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%timeit -n 1 -r 5\ndefaultmodelGBR.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%timeit -n 1 -r 5\nfinal_modelGBR.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"default_pred = defaultmodelGBR.predict(X_test)\nfinal_pred = final_modelGBR.predict(X_test)\nprint('Default model performance on the test set: MAE = %0.2f.' % mae(y_test, default_pred))\nprint('Final model performance on the test set:   MAE = %0.2f.' % mae(y_test, final_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get a sense of the predictions, we can plot the distribution of true values on the test set and the predicted values on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6)) \n\n# Density plot of the final predictions and the test values\nsns.kdeplot(final_pred, label = 'Predictions')\nsns.kdeplot(y_test, label = 'Values')\n\n# Label the plot\nplt.xlabel('Time in days'); plt.ylabel('Density');\nplt.title('Test Values and Predictions');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution is highly skewed with the density of the predicted values closer to the median of the test values rather than the actual peak. It appears the model might be less accurate at predicting the extreme values and instead predict values further away from the median"},{"metadata":{},"cell_type":"markdown","source":"Another diagnostic plot is a histogram of the residuals. Ideally, we would hope that the residuals are normally distributed, meaning that the model is wrong the same amount in both directions (high and low)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (6, 6))\n\n# Calculate the residuals \nresiduals = final_pred - y_test\n\n# Plot the residuals in a histogram\nplt.hist(residuals, color = 'green', bins = 20,\n         edgecolor = 'black')\nplt.xlabel('Error'); plt.ylabel('Count')\nplt.title('Distribution of Residuals');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The residuals are far from a  normal distribution, with noticeable outliers on both sides of the low and high end. These indicate errors where the model estimate was far below that of the true value"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(final_modelGBR, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSSIONS:**\n\n**There are a key issues that we draw from the San Francisco Building Permits data:**\n\n**1.Permit type of category 8 which involve alterations in buildings tend to matter the most and normally tend to take the least time literary in hours for the permit to be issued on average, then followed by permit type of category 3**\n\n**It's important to note that some permit types of category 2 takes the most time on average to process upto 3 years.**\n\n**It can be noted that the city's most existing construction type fall in the category of type 5.0 then followed by construction type 1.0.**\n\n# **Finally**\n \n# Using the given San Franscisco Building Permits data, a machine learning model can predict the The time taken for a permit to be processed to within 10 points.\n#     The five most important variables for determining the Time taken for a permit to be issued being:Permit Type_8,Permit Type Definition_additions alterations or repairs Permit Type, Definition_otc alterations permit,Permit Type_3  and Plansets "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}